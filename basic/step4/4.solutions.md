# Step 4: Model Persistence and Project Structure

## Objective

Learn proper techniques for model persistence and establish a well-organized project structure for your ML API.

## Context

As your ML API grows, it becomes important to organize your code and manage your models effectively. In this step, you'll learn how to properly structure your project, implement model versioning, and create a more robust model persistence system.

## Why it is required

Good project structure and model persistence practices are essential for:

- Code maintainability and reusability
- Easier collaboration with other developers
- Model versioning and tracking
- Managing different model versions for different use cases
- Reliable loading and serving of models in production
- Supporting model updates without API downtime

## How to achieve this

### 1. Create an improved project structure

```
ml-api-project/
├── app/
│   ├── __init__.py
│   ├── main.py           # FastAPI application
│   ├── models/
│   │   ├── __init__.py
│   │   └── iris_model.py  # ML model functions
│   ├── api/
│   │   ├── __init__.py
│   │   └── endpoints.py   # API route handlers
│   ├── schemas/
│   │   ├── __init__.py
│   │   └── iris.py        # Pydantic models
│   ├── services/
│   │   ├── __init__.py
│   │   └── model_service.py  # Model management
│   └── utils/
│       ├── __init__.py
│       └── helpers.py     # Utility functions
├── models/                # Directory to store saved models
│   └── iris/              # Model-specific subdirectory
│       ├── v1/            # Version subdirectory
│       │   ├── model.pkl
│       │   ├── scaler.pkl
│       │   └── metadata.json
│       └── latest/        # Symbolic link to latest version
├── data/                  # Directory for datasets
│   └── iris.csv           # Iris dataset (if needed)
├── tests/                 # Test directory
│   ├── __init__.py
│   └── test_api.py        # API tests
├── config.py              # Configuration settings
└── requirements.txt       # Dependencies
```

### 2. Create the necessary directories

```bash
# Create the directory structure
mkdir -p ml-api-project/{app/{models,api,schemas,services,utils},models/iris/v1,data,tests}
touch ml-api-project/{app/{__init__.py,main.py},app/{models,api,schemas,services,utils}/{__init__.py},tests/{__init__.py,test_api.py},config.py,requirements.txt}
```

### 3. Create the configuration file

```python
# config.py
import os
from pathlib import Path

# Base paths
BASE_DIR = Path(__file__).resolve().parent
MODELS_DIR = os.path.join(BASE_DIR, "models")
DATA_DIR = os.path.join(BASE_DIR, "data")

# Model configuration
DEFAULT_MODEL = "iris"
DEFAULT_MODEL_VERSION = "v1"

# API configuration
API_TITLE = "ML Model API"
API_DESCRIPTION = "A simple API for ML model predictions"
API_VERSION = "0.1.0"

# Server configuration
HOST = "0.0.0.0"
PORT = 8000
RELOAD = True
```

### 4. Create the model service

```python
# app/services/model_service.py
import os
import json
import pickle
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Tuple, List, Optional

import config

# Ensure the models directory exists
os.makedirs(os.path.join(config.MODELS_DIR, "iris", "v1"), exist_ok=True)

class ModelService:
    def __init__(self, model_name: str = None, version: str = None, load_existing: bool = True):
        self.model_name = model_name or config.DEFAULT_MODEL
        self.version = version or config.DEFAULT_MODEL_VERSION
        self.model = None
        self.scaler = None
        self.metadata = {}
        self.model_dir = os.path.join(config.MODELS_DIR, self.model_name, self.version)
        
        # Load the model only if requested
        if load_existing:
            self.load_model()
        
    def load_model(self) -> None:
        """Load model, scaler, and metadata from disk"""
        # If version is "latest", try to load the latest version from the text file
        if self.version == "latest":
            latest_file = os.path.join(config.MODELS_DIR, self.model_name, "latest.txt")
            if os.path.exists(latest_file):
                with open(latest_file, "r") as f:
                    self.version = f.read().strip()
                    self.model_dir = os.path.join(config.MODELS_DIR, self.model_name, self.version)
        
        # Check if model directory exists
        if not os.path.exists(self.model_dir):
            raise FileNotFoundError(f"Model directory not found: {self.model_dir}")
        
        # Load the model
        model_path = os.path.join(self.model_dir, "model.pkl")
        if os.path.exists(model_path):
            with open(model_path, "rb") as f:
                self.model = pickle.load(f)
        else:
            raise FileNotFoundError(f"Model file not found: {model_path}")
        
        # Load the scaler
        scaler_path = os.path.join(self.model_dir, "scaler.pkl")
        if os.path.exists(scaler_path):
            with open(scaler_path, "rb") as f:
                self.scaler = pickle.load(f)
        
        # Load the metadata
        metadata_path = os.path.join(self.model_dir, "metadata.json")
        if os.path.exists(metadata_path):
            with open(metadata_path, "r") as f:
                self.metadata = json.load(f)
    
    def save_model(self, model: Any, scaler: Any = None, metadata: Dict = None, 
                version: str = None) -> str:
        """Save model, scaler, and metadata to disk"""
        # If version is not provided, create a new version
        if version is None:
            # Generate a version name based on current timestamp
            version = f"v{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        # Create the model directory
        model_dir = os.path.join(config.MODELS_DIR, self.model_name, version)
        os.makedirs(model_dir, exist_ok=True)
        
        # Save the model
        model_path = os.path.join(model_dir, "model.pkl")
        with open(model_path, "wb") as f:
            pickle.dump(model, f)
        
        # Save the scaler if provided
        if scaler is not None:
            scaler_path = os.path.join(model_dir, "scaler.pkl")
            with open(scaler_path, "wb") as f:
                pickle.dump(scaler, f)
        
        # Save the metadata if provided
        if metadata is not None:
            metadata_path = os.path.join(model_dir, "metadata.json")
            with open(metadata_path, "w") as f:
                json.dump(metadata, f)
        
        # Instead of using symlinks, create a text file that points to the latest version
        latest_file = os.path.join(config.MODELS_DIR, self.model_name, "latest.txt")
        with open(latest_file, "w") as f:
            f.write(version)
        
        return version
    
    def predict(self, features: np.ndarray) -> Dict[str, Any]:
        """Make a prediction using the loaded model"""
        if self.model is None:
            raise ValueError("Model not loaded")
        
        # Scale the features if a scaler is available
        if self.scaler is not None:
            features = self.scaler.transform(features)
        
        # Make prediction
        prediction = self.model.predict(features)
        
        # Get prediction probabilities if available
        probabilities = None
        if hasattr(self.model, "predict_proba"):
            probabilities = self.model.predict_proba(features).tolist()
        
        # Get the class names if available
        target_names = self.metadata.get("target_names", ["Class " + str(i) for i in range(len(np.unique(prediction)))])
        
        # Format the response
        result = {
            "prediction": [target_names[i] for i in prediction.tolist()],
            "prediction_index": prediction.tolist(),
        }
        
        if probabilities:
            result["probabilities"] = probabilities
        
        return result
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the loaded model"""
        return {
            "model_name": self.model_name,
            "version": self.version,
            "model_type": type(self.model).__name__ if self.model else None,
            "metadata": self.metadata,
            "feature_names": self.metadata.get("feature_names", []),
            "target_names": self.metadata.get("target_names", []),
            "created_at": self.metadata.get("created_at", None),
            "accuracy": self.metadata.get("accuracy", None)
        }
    
    def get_available_versions(self) -> List[str]:
        """Get a list of available model versions"""
        model_path = os.path.join(config.MODELS_DIR, self.model_name)
        
        if not os.path.exists(model_path):
            return []
        
        # Get directories that don't start with . and aren't 'latest'
        versions = [d for d in os.listdir(model_path) 
                   if os.path.isdir(os.path.join(model_path, d)) 
                   and not d.startswith(".") 
                   and d != "latest"]
        
        return sorted(versions)
```

### 5. Create the Pydantic schemas

```python
# app/schemas/iris.py
from typing import List, Optional
from pydantic import BaseModel, Field

class IrisFeatures(BaseModel):
    sepal_length: float = Field(..., gt=0, description="Sepal length in cm")
    sepal_width: float = Field(..., gt=0, description="Sepal width in cm")
    petal_length: float = Field(..., gt=0, description="Petal length in cm")
    petal_width: float = Field(..., gt=0, description="Petal width in cm")
    
    class Config:
        schema_extra = {
            "example": {
                "sepal_length": 5.1,
                "sepal_width": 3.5,
                "petal_length": 1.4,
                "petal_width": 0.2
            }
        }

class IrisFeaturesArray(BaseModel):
    features: List[float] = Field(..., min_items=4, max_items=4, 
                               description="Array of 4 features: [sepal_length, sepal_width, petal_length, petal_width]")
    
    class Config:
        schema_extra = {
            "example": {
                "features": [5.1, 3.5, 1.4, 0.2]
            }
        }

class PredictionResponse(BaseModel):
    prediction: List[str]
    prediction_index: List[int]
    probabilities: Optional[List[List[float]]] = None

class ModelInfo(BaseModel):
    model_name: str
    version: str
    model_type: Optional[str] = None
    feature_names: List[str]
    target_names: List[str]
    created_at: Optional[str] = None
    accuracy: Optional[float] = None
```

### 6. Create the API endpoints

```python
# app/api/endpoints.py
from fastapi import APIRouter, HTTPException, Query, Depends
import numpy as np
from typing import List, Dict, Any
import config

from app.schemas.iris import (
    IrisFeatures, 
    IrisFeaturesArray, 
    PredictionResponse,
    ModelInfo
)
from app.services.model_service import ModelService

router = APIRouter()

def get_model_service(
    model_name: str = Query(None, description="Model name"),
    version: str = Query(None, description="Model version")
) -> ModelService:
    """Get a model service instance with the specified model and version"""
    try:
        # Use the parameters if provided, otherwise use defaults from config
        model_name = model_name or config.DEFAULT_MODEL
        version = version or config.DEFAULT_MODEL_VERSION
        
        # Create and return the model service
        return ModelService(model_name=model_name, version=version)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading model: {str(e)}")

@router.get("/", summary="Root endpoint", tags=["General"])
async def root():
    """Root endpoint, provides basic API information"""
    return {
        "message": "Welcome to the ML Model API",
        "docs_url": "/docs",
        "redoc_url": "/redoc"
    }

@router.get("/info", response_model=ModelInfo, summary="Get model information", tags=["Model"])
async def get_model_info(model_service: ModelService = Depends(get_model_service)):
    """Get information about the currently loaded model"""
    return model_service.get_model_info()

@router.get("/versions", summary="Get available model versions", tags=["Model"])
async def get_versions(model_service: ModelService = Depends(get_model_service)):
    """Get a list of available model versions"""
    versions = model_service.get_available_versions()
    return {"versions": versions}

@router.post("/predict", response_model=PredictionResponse, summary="Make a prediction", tags=["Prediction"])
async def predict(
    features: IrisFeatures,
    model_service: ModelService = Depends(get_model_service)
):
    """Make a prediction based on the provided iris features"""
    try:
        # Convert to numpy array in the expected format
        input_features = np.array([
            [
                features.sepal_length, 
                features.sepal_width, 
                features.petal_length, 
                features.petal_width
            ]
        ])
        
        # Make prediction
        result = model_service.predict(input_features)
        
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction error: {str(e)}")

@router.post("/predict/array", response_model=PredictionResponse, summary="Make a prediction from array", tags=["Prediction"])
async def predict_array(
    features: IrisFeaturesArray,
    model_service: ModelService = Depends(get_model_service)
):
    """Make a prediction based on a feature array [sepal_length, sepal_width, petal_length, petal_width]"""
    try:
        # Convert to numpy array in the expected format
        input_features = np.array([features.features])
        
        # Make prediction
        result = model_service.predict(input_features)
        
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction error: {str(e)}")

@router.post("/predict/batch", response_model=List[PredictionResponse], summary="Make batch predictions", tags=["Prediction"])
async def predict_batch(
    features_batch: List[IrisFeatures],
    model_service: ModelService = Depends(get_model_service)
):
    """Make predictions for a batch of samples"""
    try:
        # Convert each item in the batch to a numpy array
        input_features = np.array([
            [
                features.sepal_length, 
                features.sepal_width, 
                features.petal_length, 
                features.petal_width
            ]
            for features in features_batch
        ])
        
        # Make prediction
        result = model_service.predict(input_features)
        
        # Format for response
        results = []
        for i in range(len(input_features)):
            # Create individual results from the batch prediction
            item_result = {
                "prediction": [result["prediction"][i]],
                "prediction_index": [result["prediction_index"][i]],
            }
            
            if "probabilities" in result:
                item_result["probabilities"] = [result["probabilities"][i]]
                
            results.append(item_result)
        
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Batch prediction error: {str(e)}")
```

### 7. Create the iris model training script

```python
# app/models/iris_model.py
import os
import numpy as np
import pandas as pd
import json
from datetime import datetime
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

from app.services.model_service import ModelService
import config

def train_iris_model(save: bool = True) -> tuple:
    """
    Train an Iris classification model and optionally save it
    
    Args:
        save: Whether to save the model
        
    Returns:
        Tuple of (model, scaler, metadata, accuracy)
    """
    print("Loading Iris dataset...")
    iris = load_iris()
    X = iris.data
    y = iris.target
    feature_names = iris.feature_names
    target_names = iris.target_names
    
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Train the model
    print("Training KNN classifier...")
    model = KNeighborsClassifier(n_neighbors=5)
    model.fit(X_train_scaled, y_train)
    
    # Evaluate the model
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Model accuracy: {accuracy:.4f}")
    
    # Create metadata
    metadata = {
        "model_type": "KNeighborsClassifier",
        "feature_names": list(feature_names),
        "target_names": list(target_names),
        "params": model.get_params(),
        "accuracy": float(accuracy),
        "created_at": datetime.now().isoformat(),
        "dataset": "iris",
        "test_size": 0.3,
        "random_state": 42
    }
    
    # Save the model if requested
    if save:
        # Ensure the models directory exists
        model_dir = os.path.join(config.MODELS_DIR, "iris")
        os.makedirs(model_dir, exist_ok=True)
        
        # Use the model service to save everything
        model_service = ModelService(model_name="iris", load_existing=False)
        version = model_service.save_model(model, scaler, metadata, version="v1")
        print(f"Model saved as version: {version}")
    
    return model, scaler, metadata, accuracy

if __name__ == "__main__":
    train_iris_model(save=True)
```

### 8. Create the main application file

```python
# app/main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import logging
from contextlib import asynccontextmanager

import config
from app.api.endpoints import router as api_router

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Define lifespan context manager (replaces on_event)
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup code
    logger.info("Starting up the API server...")
    yield
    # Shutdown code
    logger.info("Shutting down the API server...")

# Create FastAPI app
app = FastAPI(
    title=config.API_TITLE,
    description=config.API_DESCRIPTION,
    version=config.API_VERSION,
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan,  # Use the new lifespan parameter
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(api_router)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app", 
        host=config.HOST, 
        port=config.PORT, 
        reload=config.RELOAD
    )
```

### 9. Create a helper script to train and save the model

```python
# train_model.py
from app.models.iris_model import train_iris_model

if __name__ == "__main__":
    train_iris_model(save=True)
```

### 10. Train the model and run the application

```bash
# First, train and save the model
python train_model.py

# Then run the application
python -m app.main
```

## Examples of usage

### Running the application

```bash
# Navigate to your project directory
cd ml-api-project

# Train and save the model
python train_model.py

# Run the application
python -m app.main
```

### Making predictions with the API

Using curl:

```bash
# Make a prediction with the named parameters
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'

# Make a prediction with array format
curl -X POST "http://localhost:8000/predict/array" \
     -H "Content-Type: application/json" \
     -d '{"features": [5.1, 3.5, 1.4, 0.2]}'

# Get model information
curl "http://localhost:8000/info"

# Get available model versions
curl "http://localhost:8000/versions"
```

Using Python requests:

```python
import requests

base_url = "http://localhost:8000"

# Get model information
response = requests.get(f"{base_url}/info")
print("Model info:", response.json())

# Make a prediction
input_data = {
    "sepal_length": 5.1,
    "sepal_width": 3.5,
    "petal_length": 1.4,
    "petal_width": 0.2
}
response = requests.post(f"{base_url}/predict", json=input_data)
print("Prediction:", response.json())

# Make a prediction with array format
array_data = {
    "features": [5.1, 3.5, 1.4, 0.2]
}
response = requests.post(f"{base_url}/predict/array", json=array_data)
print("Array prediction:", response.json())

# Use a specific model version
response = requests.get(f"{base_url}/info?version=v1")
print("Model v1 info:", response.json())
```

## Tasks for students

1. Create the project structure and implement the files as described above
2. Train and save the Iris model
3. Run the application and test the API endpoints
4. Modify the model service to:
   - Add a method to retrain the model with new parameters
   - Add a method to delete unused model versions
5. Extend the project to support multiple model types:
   - Add a new model for a different dataset (e.g., Wine or Breast Cancer from scikit-learn)
   - Modify the ModelService to handle different model types
   - Update the API to allow selecting the model type
6. Implement error handling for:
   - Missing models
   - Invalid feature inputs
   - Missing or incorrect version numbers
7. Add a simple configuration check on startup to ensure the model versions are valid

## Solution (for instructors)

### Solution for Task 4: Adding methods to ModelService

```python
def retrain_model(self, params: Dict[str, Any] = None) -> str:
    """Retrain the model with new parameters and save it"""
    from app.models.iris_model import train_iris_model
    
    # Train a new model
    model, scaler, metadata, accuracy = train_iris_model(save=False)
    
    # If params are provided, update the model parameters
    if params is not None:
        model.set_params(**params)
        # Retrain with new parameters
        from sklearn.datasets import load_iris
        from sklearn.model_selection import train_test_split
        
        iris = load_iris()
        X = iris.data
        y = iris.target
        
        # Split the data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        
        # Standardize features
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Retrain the model
        model.fit(X_train_scaled, y_train)
        
        # Update accuracy in metadata
        from sklearn.metrics import accuracy_score
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        metadata["accuracy"] = float(accuracy)
        metadata["params"] = model.get_params()
    
    # Save the model with a new version
    new_version = self.save_model(model, scaler, metadata)
    
    # Update the current instance
    self.version = new_version
    self.model = model
    self.scaler = scaler
    self.metadata = metadata
    
    return new_version

def delete_model_version(self, version: str) -> bool:
    """Delete a specific model version"""
    # Don't allow deleting the current version
    if version == self.version:
        raise ValueError(f"Cannot delete the currently loaded version: {version}")
    
    # Don't allow deleting the latest version
    latest_dir = os.path.join(config.MODELS_DIR, self.model_name, "latest")
    if os.path.islink(latest_dir):
        target_dir = os.path.realpath(latest_dir)
        if os.path.basename(os.path.dirname(target_dir)) == version:
            raise ValueError(f"Cannot delete the latest version: {version}")
    
    # Check if the version exists
    version_dir = os.path.join(config.MODELS_DIR, self.model_name, version)
    if not os.path.exists(version_dir):
        raise FileNotFoundError(f"Version not found: {version}")
    
    # Delete the version directory
    import shutil
    shutil.rmtree(version_dir)
    
    return True
```

### Solution for Task 7: Startup configuration check

Add this to `app/main.py`:

```python
@app.on_event("startup")
async def startup_event():
    logger.info("Starting up the API server...")
    
    # Check if models directory exists
    if not os.path.exists(config.MODELS_DIR):
        logger.warning(f"Models directory not found: {config.MODELS_DIR}")
        logger.info("Creating models directory...")
        os.makedirs(config.MODELS_DIR, exist_ok=True)
    
    # Check if default model exists
    model_dir = os.path.join(config.MODELS_DIR, config.DEFAULT_MODEL)
    if not os.path.exists(model_dir):
        logger.warning(f"Default model directory not found: {model_dir}")
        logger.info("Training and saving default model...")
        from app.models.iris_model import train_iris_model
        train_iris_model(save=True)
    
    # Check if default version exists
    version_dir = os.path.join(model_dir, config.DEFAULT_MODEL_VERSION)
    if not os.path.exists(version_dir):
        logger.warning(f"Default model version not found: {version_dir}")
        logger.info("Training and saving default model version...")
        from app.models.iris_model import train_iris_model
        train_iris_model(save=True)
    
    logger.info("API server started successfully")
```

## Next steps

In the next step, we'll learn about input validation and error handling to make our API more robust and user-friendly.