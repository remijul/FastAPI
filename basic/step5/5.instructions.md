# Step 5: Input Validation and Error Handling

## Objective

Implement robust input validation and error handling for your ML API to make it more reliable and user-friendly.

## Context

Error handling and input validation are crucial for creating a production-ready API. In this step, you'll learn how to validate user inputs, handle various types of errors, and provide meaningful feedback to users.

## Why it is required

Proper input validation and error handling:

- Prevents invalid inputs from causing unexpected behaviors
- Improves security by rejecting potentially harmful inputs
- Provides clear feedback to users about what went wrong
- Makes debugging easier by providing meaningful error messages
- Enhances the overall reliability and usability of your API

## How to achieve this

### 1. Enhance your Pydantic models with validation

Update `app/schemas/iris.py` with more robust validation:

```python
from typing import List, Optional, Union, Dict, Any
from pydantic import BaseModel, Field, validator, root_validator
import numpy as np

class IrisFeatures(BaseModel):
    sepal_length: float = Field(..., gt=0, le=30, description="Sepal length in cm (0-30cm)")
    sepal_width: float = Field(..., gt=0, le=30, description="Sepal width in cm (0-30cm)")
    petal_length: float = Field(..., gt=0, le=30, description="Petal length in cm (0-30cm)")
    petal_width: float = Field(..., gt=0, le=30, description="Petal width in cm (0-30cm)")
    
    class Config:
        schema_extra = {
            "example": {
                "sepal_length": 5.1,
                "sepal_width": 3.5,
                "petal_length": 1.4,
                "petal_width": 0.2
            }
        }
    
    @validator('*')
    def check_nan_inf(cls, v, field):
        if isinstance(v, float):
            if np.isnan(v) or np.isinf(v):
                raise ValueError(f"{field.name} must be a valid number (not NaN or infinity)")
        return v

    @root_validator
    def check_realistic_values(cls, values):
        """Validate that values are within realistic ranges for iris flowers"""
        if all(key in values for key in ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']):
            sepal_length = values['sepal_length']
            sepal_width = values['sepal_width']
            petal_length = values['petal_length']
            petal_width = values['petal_width']
            
            # Check if sepal length is always greater than petal length (common for iris)
            if sepal_length < petal_length:
                raise ValueError("Sepal length is typically greater than petal length for iris flowers")
            
            # Check if the combination of values makes biological sense
            if petal_length < 0.5 and petal_width > 0.5:
                raise ValueError("Unusual combination: very short petals are not typically wide")
                
            if sepal_length > 10:
                raise ValueError("Unusually large sepal length (>10cm). Verify your measurement.")
                
        return values

class IrisFeaturesArray(BaseModel):
    features: List[float] = Field(..., min_items=4, max_items=4, 
                               description="Array of 4 features: [sepal_length, sepal_width, petal_length, petal_width]")
    
    class Config:
        schema_extra = {
            "example": {
                "features": [5.1, 3.5, 1.4, 0.2]
            }
        }
    
    @validator('features')
    def validate_features(cls, v):
        if len(v) != 4:
            raise ValueError("Features array must contain exactly 4 values")
        
        # Check individual feature values
        for i, feature in enumerate(v):
            if np.isnan(feature) or np.isinf(feature):
                raise ValueError(f"Feature {i+1} must be a valid number (not NaN or infinity)")
            
            if feature <= 0 or feature > 30:
                raise ValueError(f"Feature {i+1} must be between 0 and 30 cm")
        
        # Perform same biological checks as in IrisFeatures
        sepal_length, sepal_width, petal_length, petal_width = v
        
        if sepal_length < petal_length:
            raise ValueError("Sepal length is typically greater than petal length for iris flowers")
            
        if petal_length < 0.5 and petal_width > 0.5:
            raise ValueError("Unusual combination: very short petals are not typically wide")
            
        if sepal_length > 10:
            raise ValueError("Unusually large sepal length (>10cm). Verify your measurement.")
            
        return v

class PredictionResponse(BaseModel):
    prediction: List[str]
    prediction_index: List[int]
    probabilities: Optional[List[List[float]]] = None
    
    class Config:
        schema_extra = {
            "example": {
                "prediction": ["setosa"],
                "prediction_index": [0],
                "probabilities": [[0.95, 0.04, 0.01]]
            }
        }

class ModelInfo(BaseModel):
    model_name: str
    version: str
    model_type: Optional[str] = None
    feature_names: List[str]
    target_names: List[str]
    created_at: Optional[str] = None
    accuracy: Optional[float] = None
    
    class Config:
        schema_extra = {
            "example": {
                "model_name": "iris",
                "version": "v1",
                "model_type": "KNeighborsClassifier",
                "feature_names": ["sepal length (cm)", "sepal width (cm)", "petal length (cm)", "petal width (cm)"],
                "target_names": ["setosa", "versicolor", "virginica"],
                "created_at": "2023-01-01T12:00:00",
                "accuracy": 0.97
            }
        }

# New schemas for error handling
class ErrorResponse(BaseModel):
    detail: Union[str, Dict[str, Any]]
    status_code: int
    
    class Config:
        schema_extra = {
            "example": {
                "detail": "Invalid input: sepal_length must be greater than 0",
                "status_code": 400
            }
        }
```

### 2. Create custom exception handlers

Create a new file `app/utils/exceptions.py`:

```python
from fastapi import HTTPException, Request
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
import logging
from typing import Union, Dict, Any

logger = logging.getLogger(__name__)

class ModelNotFoundError(HTTPException):
    def __init__(self, model_name: str, version: str = None):
        detail = f"Model '{model_name}' not found"
        if version:
            detail += f" with version '{version}'"
        super().__init__(status_code=404, detail=detail)

class InvalidInputError(HTTPException):
    def __init__(self, detail: str):
        super().__init__(status_code=400, detail=f"Invalid input: {detail}")

class PredictionError(HTTPException):
    def __init__(self, detail: str):
        super().__init__(status_code=500, detail=f"Prediction failed: {detail}")

async def http_exception_handler(request: Request, exc: HTTPException):
    """Handler for HTTPException"""
    logger.error(f"HTTP error: {exc.status_code} - {exc.detail}")
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail, "status_code": exc.status_code}
    )

async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Handler for RequestValidationError"""
    # Extract the validation errors
    error_details = []
    for error in exc.errors():
        location = ".".join(str(loc) for loc in error["loc"])
        message = error["msg"]
        error_details.append(f"{location}: {message}")
    
    detailed_error = "\n".join(error_details)
    logger.error(f"Validation error: {detailed_error}")
    
    return JSONResponse(
        status_code=422,
        content={
            "detail": {
                "errors": exc.errors(),
                "message": "Input validation error. Check your request format."
            },
            "status_code": 422
        }
    )

async def general_exception_handler(request: Request, exc: Exception):
    """Handler for general exceptions"""
    logger.exception("Unhandled exception", exc_info=exc)
    return JSONResponse(
        status_code=500,
        content={
            "detail": f"Internal server error: {str(exc)}",
            "status_code": 500
        }
    )
```

### 3. Update the API endpoints with improved error handling

Update `app/api/endpoints.py`:

```python
from fastapi import APIRouter, HTTPException, Query, Depends, Path
import numpy as np
from typing import List, Dict, Any, Optional

from app.schemas.iris import (
    IrisFeatures, 
    IrisFeaturesArray, 
    PredictionResponse,
    ModelInfo,
    ErrorResponse
)
from app.services.model_service import ModelService
from app.utils.exceptions import ModelNotFoundError, InvalidInputError, PredictionError

router = APIRouter()

def get_model_service(
    model_name: str = Query(None, description="Model name"),
    version: str = Query(None, description="Model version")
) -> ModelService:
    """Get a model service instance with the specified model and version"""
    try:
        # Use the parameters if provided, otherwise use defaults
        model_name = model_name or None
        version = version or None
        
        # Create and return the model service
        return ModelService(model_name=model_name, version=version)
    except FileNotFoundError as e:
        raise ModelNotFoundError(
            model_name=model_name or "default", 
            version=version or "default"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading model: {str(e)}")

@router.get(
    "/", 
    summary="Root endpoint", 
    tags=["General"],
    responses={
        200: {"description": "Successful response"}
    }
)
async def root():
    """Root endpoint, provides basic API information"""
    return {
        "message": "Welcome to the ML Model API",
        "docs_url": "/docs",
        "redoc_url": "/redoc"
    }

@router.get(
    "/info", 
    response_model=ModelInfo, 
    summary="Get model information", 
    tags=["Model"],
    responses={
        200: {"description": "Model information retrieved successfully"},
        404: {"description": "Model not found", "model": ErrorResponse},
        500: {"description": "Internal server error", "model": ErrorResponse}
    }
)
async def get_model_info(model_service: ModelService = Depends(get_model_service)):
    """Get information about the currently loaded model"""
    return model_service.get_model_info()

@router.get(
    "/versions", 
    summary="Get available model versions", 
    tags=["Model"],
    responses={
        200: {"description": "Versions retrieved successfully"},
        404: {"description": "Model not found", "model": ErrorResponse},
        500: {"description": "Internal server error", "model": ErrorResponse}
    }
)
async def get_versions(model_service: ModelService = Depends(get_model_service)):
    """Get a list of available model versions"""
    versions = model_service.get_available_versions()
    return {"versions": versions}

@router.post(
    "/predict", 
    response_model=PredictionResponse, 
    summary="Make a prediction", 
    tags=["Prediction"],
    responses={
        200: {"description": "Prediction successful"},
        400: {"description": "Invalid input", "model": ErrorResponse},
        404: {"description": "Model not found", "model": ErrorResponse},
        422: {"description": "Validation error", "model": ErrorResponse},
        500: {"description": "Prediction error", "model": ErrorResponse}
    }
)
async def predict(
    features: IrisFeatures,
    model_service: ModelService = Depends(get_model_service)
):
    """
    Make a prediction based on the provided iris features
    
    - **sepal_length**: Length of the sepal in cm (must be > 0)
    - **sepal_width**: Width of the sepal in cm (must be > 0)
    - **petal_length**: Length of the petal in cm (must be > 0)
    - **petal_width**: Width of the petal in cm (must be > 0)
    """
    try:
        # Convert to numpy array in the expected format
        input_features = np.array([
            [
                features.sepal_length, 
                features.sepal_width, 
                features.petal_length, 
                features.petal_width
            ]
        ])
        
        # Perform additional validation
        if np.any(input_features < 0):
            raise InvalidInputError("All measurements must be positive")
        
        # Make prediction
        result = model_service.predict(input_features)
        
        return result
    except InvalidInputError:
        # Re-raise the caught exception
        raise
    except ValueError as e:
        # Convert ValueError to a more friendly error
        raise InvalidInputError(str(e))
    except Exception as e:
        # General prediction error
        raise PredictionError(str(e))

@router.post(
    "/predict/array", 
    response_model=PredictionResponse, 
    summary="Make a prediction from array", 
    tags=["Prediction"],
    responses={
        200: {"description": "Prediction successful"},
        400: {"description": "Invalid input", "model": ErrorResponse},
        404: {"description": "Model not found", "model": ErrorResponse},
        422: {"description": "Validation error", "model": ErrorResponse},
        500: {"description": "Prediction error", "model": ErrorResponse}
    }
)
async def predict_array(
    features: IrisFeaturesArray,
    model_service: ModelService = Depends(get_model_service)
):
    """Make a prediction based on a feature array [sepal_length, sepal_width, petal_length, petal_width]"""
    try:
        # Convert to numpy array in the expected format
        input_features = np.array([features.features])
        
        # Perform additional validation
        if np.any(input_features < 0):
            raise InvalidInputError("All measurements must be positive")
        
        # Make prediction
        result = model_service.predict(input_features)
        
        return result
    except InvalidInputError:
        # Re-raise the caught exception
        raise
    except ValueError as e:
        # Convert ValueError to a more friendly error
        raise InvalidInputError(str(e))
    except Exception as e:
        # General prediction error
        raise PredictionError(str(e))

@router.post(
    "/predict/batch", 
    response_model=List[PredictionResponse], 
    summary="Make batch predictions", 
    tags=["Prediction"],
    responses={
        200: {"description": "Batch prediction successful"},
        400: {"description": "Invalid input", "model": ErrorResponse},
        404: {"description": "Model not found", "model": ErrorResponse},
        422: {"description": "Validation error", "model": ErrorResponse},
        500: {"description": "Prediction error", "model": ErrorResponse}
    }
)
async def predict_batch(
    features_batch: List[IrisFeatures],
    model_service: ModelService = Depends(get_model_service)
):
    """Make predictions for a batch of samples"""
    try:
        # Check for empty batch
        if not features_batch:
            raise InvalidInputError("Batch cannot be empty")
            
        # Convert each item in the batch to a numpy array
        input_features = np.array([
            [
                features.sepal_length, 
                features.sepal_width, 
                features.petal_length, 
                features.petal_width
            ]
            for features in features_batch
        ])
        
        # Limit batch size for performance reasons
        if len(input_features) > 100:
            raise InvalidInputError("Batch size cannot exceed 100 samples")
        
        # Perform additional validation
        if np.any(input_features < 0):
            raise InvalidInputError("All measurements must be positive")
        
        # Make prediction
        result = model_service.predict(input_features)
        
        # Format for response
        results = []
        for i in range(len(input_features)):
            # Create individual results from the batch prediction
            item_result = {
                "prediction": [result["prediction"][i]],
                "prediction_index": [result["prediction_index"][i]],
            }
            
            if "probabilities" in result:
                item_result["probabilities"] = [result["probabilities"][i]]
                
            results.append(item_result)
        
        return results
    except InvalidInputError:
        # Re-raise the caught exception
        raise
    except ValueError as e:
        # Convert ValueError to a more friendly error
        raise InvalidInputError(str(e))
    except Exception as e:
        # General prediction error
        raise PredictionError(str(e))

@router.get(
    "/health", 
    summary="Check API health", 
    tags=["General"],
    responses={
        200: {"description": "API is healthy"},
        500: {"description": "API is unhealthy", "model": ErrorResponse}
    }
)
async def health_check():
    """Check the health status of the API"""
    try:
        # Try to load the default model to check if it's available
        model_service = ModelService()
        model_info = model_service.get_model_info()
        
        return {
            "status": "healthy",
            "model": model_info["model_name"],
            "version": model_info["version"],
            "model_type": model_info["model_type"]
        }
    except Exception as e:
        # If there's an error loading the model, the API is not healthy
        raise HTTPException(
            status_code=500, 
            detail={
                "status": "unhealthy",
                "error": str(e)
            }
        )
```

### 4. Update the main application to use the exception handlers

Update `app/main.py` to include the exception handlers:

```python
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.exceptions import RequestValidationError
import logging

import config
from app.api.endpoints import router as api_router
from app.utils.exceptions import (
    http_exception_handler, 
    validation_exception_handler,
    general_exception_handler
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("api.log")
    ]
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title=config.API_TITLE,
    description=config.API_DESCRIPTION,
    version=config.API_VERSION,
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add exception handlers
app.add_exception_handler(HTTPException, http_exception_handler)
app.add_exception_handler(RequestValidationError, validation_exception_handler)
app.add_exception_handler(Exception, general_exception_handler)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Add request logging middleware
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log all incoming requests and their processing time"""
    import time
    
    start_time = time.time()
    
    # Get request details
    method = request.method
    url = request.url.path
    query_params = str(request.query_params)
    client_host = request.client.host if request.client else "unknown"
    
    logger.info(f"Request: {method} {url} - Client: {client_host} - Params: {query_params}")
    
    # Process the request
    response = await call_next(request)
    
    # Calculate processing time
    process_time = time.time() - start_time
    formatted_process_time = "{:.2f}".format(process_time * 1000)
    
    logger.info(f"Response: {method} {url} - Status: {response.status_code} - Time: {formatted_process_time}ms")
    
    return response

# Include routers
app.include_router(api_router)

# Startup event
@app.on_event("startup")
async def startup_event():
    import os
    logger.info("Starting up the API server...")
    
    # Check if models directory exists
    if not os.path.exists(config.MODELS_DIR):
        logger.warning(f"Models directory not found: {config.MODELS_DIR}")
        logger.info("Creating models directory...")
        os.makedirs(config.MODELS_DIR, exist_ok=True)
    
    # Check if default model exists
    model_dir = os.path.join(config.MODELS_DIR, config.DEFAULT_MODEL)
    if not os.path.exists(model_dir):
        logger.warning(f"Default model directory not found: {model_dir}")
        logger.info("Training and saving default model...")
        from app.models.iris_model import train_iris_model
        train_iris_model(save=True)
    
    logger.info("API server started successfully")

# Shutdown event
@app.on_event("shutdown")
async def shutdown_event():
    logger.info("Shutting down the API server...")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app", 
        host=config.HOST, 
        port=config.PORT, 
        reload=config.RELOAD
    )
```

### 5. Add rate limiting

Create a new file `app/utils/rate_limit.py`:

```python
from fastapi import Request, HTTPException
import time
from typing import Dict, List, Tuple

class RateLimiter:
    def __init__(self, max_requests: int = 10, window_seconds: int = 60):
        """
        Initialize a rate limiter
        
        Args:
            max_requests: Maximum number of requests allowed per window
            window_seconds: Time window in seconds
        """
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.requests: Dict[str, List[float]] = {}  # client_id -> list of timestamps
    
    def is_rate_limited(self, request: Request) -> bool:
        """
        Check if a request is rate limited
        
        Args:
            request: The incoming request
            
        Returns:
            True if the request should be rate limited, False otherwise
        """
        # Get client identifier (IP address in this simple implementation)
        client_id = request.client.host if request.client else "unknown"
        
        # Get current time
        current_time = time.time()
        
        # Initialize client if not seen before
        if client_id not in self.requests:
            self.requests[client_id] = []
        
        # Remove requests older than the window
        self.requests[client_id] = [
            timestamp for timestamp in self.requests[client_id]
            if current_time - timestamp < self.window_seconds
        ]
        
        # Check if client has exceeded rate limit
        if len(self.requests[client_id]) >= self.max_requests:
            return True
        
        # Record this request
        self.requests[client_id].append(current_time)
        
        return False

# Create a global rate limiter instance
rate_limiter = RateLimiter(max_requests=60, window_seconds=60)  # 60 requests per minute
```

Update the `app/main.py` file to use the rate limiter:

```python
from app.utils.rate_limit import rate_limiter

# Add rate limiting middleware (add this before other middleware)
@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    """Apply rate limiting to all requests"""
    # Skip rate limiting for certain paths
    if request.url.path in ["/health", "/docs", "/redoc", "/openapi.json"]:
        return await call_next(request)
    
    # Check if request is rate limited
    if rate_limiter.is_rate_limited(request):
        raise HTTPException(
            status_code=429,
            detail="Too many requests. Please try again later."
        )
    
    return await call_next(request)
```

### 6. Add input sanitization helper

Create a new file `app/utils/helpers.py`:

```python
import numpy as np

def sanitize_input_features(features: np.ndarray) -> np.ndarray:
    """
    Sanitize and validate input features
    
    Args:
        features: Input feature array
        
    Returns:
        Sanitized features
        
    Raises:
        ValueError: If input is invalid
    """
    # Check if input is a numpy array
    if not isinstance(features, np.ndarray):
        raise ValueError("Input must be a numpy array")
    
    # Check dimensions
    if features.ndim != 2:
        raise ValueError(f"Expected 2D array, got {features.ndim}D")
    
    if features.shape[1] != 4:
        raise ValueError(f"Expected 4 features, got {features.shape[1]}")
    
    # Check for NaN or infinity
    if np.any(np.isnan(features)) or np.any(np.isinf(features)):
        raise ValueError("Input contains NaN or infinity values")
    
    # Check for negatives
    if np.any(features < 0):
        raise ValueError("Input contains negative values")
    
    # Check for unreasonably large values
    if np.any(features > 30):
        raise ValueError("Input contains unreasonably large values (>30cm)")
    
    # Ensure correct data type (float)
    return features.astype(np.float32)
```

Update the ModelService to use the sanitization helper:

```python
# In app/services/model_service.py
from app.utils.helpers import sanitize_input_features

def predict(self, features: np.ndarray) -> Dict[str, Any]:
    """Make a prediction using the loaded model"""
    if self.model is None:
        raise ValueError("Model not loaded")
    
    # Sanitize and validate input
    features = sanitize_input_features(features)
    
    # Scale the features if a scaler is available
    if self.scaler is not None:
        features = self.scaler.transform(features)
    
    # Make prediction
    prediction = self.model.predict(features)
    
    # Rest of the method remains the same...
```

## Examples of usage

### Testing input validation

```python
import requests

base_url = "http://localhost:8000"

# Valid input
valid_input = {
    "sepal_length": 5.1,
    "sepal_width": 3.5,
    "petal_length": 1.4,
    "petal_width": 0.2
}

# Invalid inputs to test validation
invalid_inputs = [
    # Negative value
    {
        "sepal_length": -5.1,
        "sepal_width": 3.5,
        "petal_length": 1.4,
        "petal_width": 0.2
    },
    # Missing field
    {
        "sepal_length": 5.1,
        "sepal_width": 3.5,
        "petal_width": 0.2
    },
    # Extra field
    {
        "sepal_length": 5.1,
        "sepal_width": 3.5,
        "petal_length": 1.4,
        "petal_width": 0.2,
        "extra_field": "value"
    },
    # Non-numeric value
    {
        "sepal_length": "not_a_number",
        "sepal_width": 3.5,
        "petal_length": 1.4,
        "petal_width": 0.2
    },
    # Biologically invalid (sepal length < petal length)
    {
        "sepal_length": 1.0,
        "sepal_width": 3.5,
        "petal_length": 5.0,
        "petal_width": 0.2
    }
]

# Test valid input
print("Testing valid input:")
response = requests.post(f"{base_url}/predict", json=valid_input)
print(f"Status code: {response.status_code}")
print(f"Response: {response.json()}")
print()

# Test invalid inputs
for i, invalid_input in enumerate(invalid_inputs):
    print(f"Testing invalid input {i+1}:")
    response = requests.post(f"{base_url}/predict", json=invalid_input)
    print(f"Status code: {response.status_code}")
    print(f"Response: {response.json()}")
    print()
```

### Testing error handling

```python
import requests

base_url = "http://localhost:8000"

# Test health endpoint
print("Testing health endpoint:")
response = requests.get(f"{base_url}/health")
print(f"Status code: {response.status_code}")
print(f"Response: {response.json()}")
print()

# Test non-existent model version
print("Testing non-existent model version:")
response = requests.get(f"{base_url}/info?version=non_existent")
print(f"Status code: {response.status_code}")
print(f"Response: {response.json()}")
print()

# Test rate limiting (in a loop)
print("Testing rate limiting (sending many requests):")
for i in range(65):  # More than our rate limit (60 per minute)
    response = requests.get(f"{base_url}/info")
    print(f"Request {i+1}: Status code: {response.status_code}")
    if response.status_code == 429:
        print(f"Rate limited after {i+1} requests")
        print(f"Response: {response.json()}")
        break
```

## Tasks for students

1. Implement the enhanced Pydantic models with validation
2. Create the custom exception handlers and update the main application to use them
3. Add input sanitization for the prediction endpoints
4. Implement rate limiting for the API
5. Test all error scenarios and validation rules

Optional:
6. Add additional validation rules specific to your understanding of the iris dataset
7. Implement a logging system that records errors to a file for later analysis
